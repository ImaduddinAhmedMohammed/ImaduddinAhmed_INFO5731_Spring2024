{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaduddinAhmedMohammed/ImaduddinAhmed_INFO5731_Spring2024/blob/main/Mohammed_Imad_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "\"\"\"Research Question: To find the productivity and job satisfaction of employee\n",
        "in the tech industry?\n",
        "\n",
        "For this research question, the data that needs to be collected is:\n",
        "\n",
        "Employee Productivity Metrics:\n",
        "Number of tasks completed per day/week/month\n",
        "Time taken to complete tasks\n",
        "Quality of work (e.g., error rates, customer satisfaction ratings)\n",
        "Self-reported productivity levels (e.g., using surveys)\n",
        "\n",
        "Employee Job Satisfaction Metrics:\n",
        "Responses to job satisfaction surveys (e.g., Likert scale questions)\n",
        "Turnover rates\n",
        "Attendance records\n",
        "Feedback from performance reviews or one-on-one meetings\n",
        "Employee engagement scores\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Employee Productivity Metrics\n",
        "def generate_productivity_data(num_employees):\n",
        "    productivity_data = []\n",
        "    for _ in range(num_employees):\n",
        "        tasks_completed = random.randint(20, 100)\n",
        "        time_taken = round(random.uniform(4, 10), 2)  # Hours\n",
        "        error_rate = round(random.uniform(0, 5), 2)  # Percentage\n",
        "        customer_satisfaction = round(random.uniform(3, 5), 2)  # Scale of 1-5\n",
        "        self_reported_productivity = round(random.uniform(3, 5), 2)  # Scale of 1-5\n",
        "        productivity_data.append({\n",
        "            'Tasks Completed': tasks_completed,\n",
        "            'Time Taken (hours)': time_taken,\n",
        "            'Error Rate (%)': error_rate,\n",
        "            'Customer Satisfaction': customer_satisfaction,\n",
        "            'Self-reported Productivity': self_reported_productivity\n",
        "        })\n",
        "    return pd.DataFrame(productivity_data)\n",
        "\n",
        "# Employee Job Satisfaction Metrics\n",
        "def generate_satisfaction_data(num_employees):\n",
        "    satisfaction_data = []\n",
        "    for _ in range(num_employees):\n",
        "        satisfaction_score = random.randint(1, 5)\n",
        "        turnover = random.choice(['Low', 'Medium', 'High'])\n",
        "        attendance = round(random.uniform(0.7, 1), 2)  # Percentage\n",
        "        feedback_score = round(random.uniform(3, 5), 2)  # Scale of 1-5\n",
        "        engagement_score = round(random.uniform(3, 5), 2)  # Scale of 1-5\n",
        "        satisfaction_data.append({\n",
        "            'Satisfaction Score': satisfaction_score,\n",
        "            'Turnover': turnover,\n",
        "            'Attendance': attendance,\n",
        "            'Feedback Score': feedback_score,\n",
        "            'Engagement Score': engagement_score\n",
        "        })\n",
        "    return pd.DataFrame(satisfaction_data)\n",
        "\n",
        "\n",
        "\n",
        "# Generate dataset\n",
        "num_employees = 1000\n",
        "productivity_df = generate_productivity_data(num_employees)\n",
        "satisfaction_df = generate_satisfaction_data(num_employees)\n",
        "\n",
        "employee_data = pd.concat([productivity_df, satisfaction_df,], axis=1)\n",
        "\n",
        "print(employee_data)\n",
        "\n",
        "employee_data.to_csv('employee_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf89d23f-3e05-40da-db0f-409e65870156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Tasks Completed  Time Taken (hours)  Error Rate (%)  \\\n",
            "0                 70                8.83            2.52   \n",
            "1                 20                5.75            1.87   \n",
            "2                 81                8.16            0.13   \n",
            "3                 35                7.51            4.40   \n",
            "4                 85                7.47            0.07   \n",
            "..               ...                 ...             ...   \n",
            "995               82                9.13            4.55   \n",
            "996               47                4.23            1.03   \n",
            "997               98                6.18            4.43   \n",
            "998               51                6.78            4.68   \n",
            "999               23                5.18            5.00   \n",
            "\n",
            "     Customer Satisfaction  Self-reported Productivity  Satisfaction Score  \\\n",
            "0                     3.29                        4.26                   2   \n",
            "1                     4.72                        3.19                   4   \n",
            "2                     4.55                        4.91                   1   \n",
            "3                     3.04                        4.03                   2   \n",
            "4                     3.11                        4.01                   2   \n",
            "..                     ...                         ...                 ...   \n",
            "995                   3.05                        3.43                   3   \n",
            "996                   3.47                        3.62                   2   \n",
            "997                   4.90                        3.06                   2   \n",
            "998                   3.46                        3.86                   1   \n",
            "999                   3.97                        3.03                   2   \n",
            "\n",
            "    Turnover  Attendance  Feedback Score  Engagement Score  \n",
            "0        Low        0.96            3.62              3.86  \n",
            "1       High        0.73            4.49              4.63  \n",
            "2        Low        0.88            4.97              3.29  \n",
            "3     Medium        0.75            4.70              4.37  \n",
            "4     Medium        0.99            4.51              3.49  \n",
            "..       ...         ...             ...               ...  \n",
            "995      Low        0.86            4.36              4.26  \n",
            "996   Medium        0.87            4.26              4.08  \n",
            "997   Medium        0.91            4.60              3.75  \n",
            "998     High        0.88            4.44              4.89  \n",
            "999      Low        0.90            4.45              4.83  \n",
            "\n",
            "[1000 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def fetch_article_info(url):\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    title = soup.find('h3', {'class': 'gt_rt'}).text.strip()\n",
        "    venue = soup.find('div', {'class': 'gs_a'}).text.split('-')[0].strip()\n",
        "    year = re.findall(r'\\b(?:20[12][0-9])\\b', soup.text)[0] # Extracts the year from the text\n",
        "    authors = [author.text for author in soup.find_all('div', {'class': 'gs_a'})[0].find_all('a')]\n",
        "    abstract = soup.find('div', {'id': 'abstract'}).text.strip()\n",
        "\n",
        "    article_info = {\n",
        "        'Title': title,\n",
        "        'Venue': venue,\n",
        "        'Year': year,\n",
        "        'Authors': authors,\n",
        "        'Abstract': abstract\n",
        "    }\n",
        "    return article_info\n",
        "\n",
        "def scrape_google_scholar(keyword, num_articles):\n",
        "\n",
        "    base_url = f'https://scholar.google.com/scholar?start=20&q={keyword}&hl=en&as_sdt=0,44&num={num_articles}'\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "    response = requests.get(base_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    article_links = soup.find_all('h3', {'class': 'gs_rt'})[:num_articles]\n",
        "    articles_data = []\n",
        "    for link in article_links:\n",
        "        try:\n",
        "            article_url = link.a['href']\n",
        "            article_info = fetch_article_info(article_url)\n",
        "            articles_data.append(article_info)\n",
        "            time.sleep(2)  # Adding a delay to avoid overwhelming the server\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing: {link.text.strip()}\")\n",
        "            print(e)\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(articles_data)\n",
        "\n",
        "# Scrape 1000 articles with keyword \"XYZ\"\n",
        "keyword = \"XYZ\"\n",
        "num_articles = 1000\n",
        "articles_df = scrape_google_scholar(keyword, num_articles)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "#articles_df.to_csv(\"google_scholar_articles.csv\", index=False)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(articles_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sg5dEgv9bwMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c73a5fc-2d2a-4a1e-9332-9a071cf1e7e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "29759142-7fa7-4f1c-fa21-58d7d434d0e5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'API' object has no attribute 'search'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-085e63275ee0>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtweets_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhashtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"extended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     tweet_data = {\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m\"Username\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'search'"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "# I did not get a way to use free API.\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "consumer_key = \"YOUR_CONSUMER_KEY\"\n",
        "consumer_secret = \"YOUR_CONSUMER_SECRET\"\n",
        "access_token = \"YOUR_ACCESS_TOKEN\"\n",
        "access_token_secret = \"YOUR_ACCESS_TOKEN_SECRET\"\n",
        "\n",
        "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "hashtag = \"#python\"\n",
        "\n",
        "tweets_data = []\n",
        "for tweet in tweepy.Cursor(api.search, q=hashtag, lang=\"en\", tweet_mode=\"extended\").items(100):\n",
        "    tweet_data = {\n",
        "        \"Username\": tweet.user.screen_name,\n",
        "        \"Text\": tweet.full_text,\n",
        "        \"Retweets\": tweet.retweet_count,\n",
        "        \"Likes\": tweet.favorite_count\n",
        "    }\n",
        "    tweets_data.append(tweet_data)\n",
        "\n",
        "df = pd.DataFrame(tweets_data)\n",
        "\n",
        "df.to_csv(\"twitter_data.csv\", index=False)\n",
        "\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "\n",
        "Learning Experience:\n",
        "Working on web scraping tasks is a  journey where I am learning how to\n",
        "extract valuable data from the vast ocean of information available online.\n",
        "One of the most eye-opening aspects was understanding how web pages are structured\n",
        "in HTML and how to navigate through them to find the data I need. Although I am\n",
        "still learning on how to do it, I find it really interesting and challenging\n",
        "Learning about tools like BeautifulSoup and Selenium was particularly helpful.\n",
        "BeautifulSoup is a bit simpler than the rest, so it is easy to understand.\n",
        "\n",
        "Challenges Encountered:\n",
        "Scraping data from websites that load content dynamically using JavaScript.\n",
        "It is a bit difficult to exactly pinpoint where you need to extract the data from\n",
        "using the inspect element. And hence it is difficult to implement multiple things\n",
        "from multiple packages at the same time.\n",
        "\n",
        "Relevance to Your Field of Study:\n",
        "Being a student from Data Science, knowing how to Web scrape is a requirement.\n",
        "Since extracting data and working on it is the basic function of Data Scientists,\n",
        "it is a compulsion to know how to webscrape.\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}